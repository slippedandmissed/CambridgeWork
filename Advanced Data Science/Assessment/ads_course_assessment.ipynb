{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1e4a6f",
   "metadata": {
    "id": "0b1e4a6f"
   },
   "source": [
    "# Assessment for Advanced Data Science\n",
    "\n",
    "## Christian Cabrera, Carl Henrik Ek and Neil D. Lawrence\n",
    "\n",
    "### 29th October 2021\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7856d1b",
   "metadata": {
    "id": "d7856d1b"
   },
   "source": [
    "Welcome to the course assessment for the Advanced Data Science unit. In this assessment you will build a prediction system for UK house prices. \n",
    "\n",
    "Your prediction system will be based on data from the UK Price Paid data available [here](https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads). By combining this data with the UK Office for National Statistics data on the latitude/longitude of postcodes (available [here](https://www.getthedata.com/open-postcode-geo)) you will have a record of house prices and their approximate latitude/longitude. Due to the size of these data you will use a relational database to handle them.  \n",
    "\n",
    "To make predictions of the house price you will augment your data with information obtained from Open Street Map: an open license source of mapping information. You will use the techniques you have learnt in the course to indentify and incorporate useful features for house price prediction.\n",
    "\n",
    "\n",
    "\n",
    "Alongside your implementation you will provide a short repository overview describing how you have implemented the different parts of the project and where you have placed those parts in your code repository. You will submit your code alongside a version of this notebook that will allow your examiner to understand and reconstruct the thinking behind your analysis. This notebook is structured to help you in creating that description and allow you to understand how we will allocate the marks. You should make use of the Fynesse framework (<https://github.com/lawrennd/fynesse_template>) for structuring your code. \n",
    "\n",
    "Remember the notebook you create should *tell a story*, any code that is not critical to that story can safely be placed into the associated analysis library and imported for use (structured as given in the Fynesse template)\n",
    "\n",
    "The maximum total mark for this assessment is 20. That mark is split into Three Questions below, each worth 5 marks each. Then a final 5 marks will be given for the quality, structure and reusability of the code and analysis you produce giving 20 marks in total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e7c53",
   "metadata": {
    "id": "bb7e7c53"
   },
   "source": [
    "### Useful Links\n",
    "\n",
    "You may find some of the following links useful when building your system.\n",
    "\n",
    "University instuctions on Security and Privacy with AWS.\n",
    "\n",
    "https://help.uis.cam.ac.uk/service/network-services/hosting-services/AWS/aws-security-privacy\n",
    "\n",
    "Security Rules in AWS \n",
    "\n",
    "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.Scenarios.html#USER_VPC.Scenario4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ee6ce",
   "metadata": {
    "id": "9d3ee6ce"
   },
   "source": [
    "### Installing Your Library\n",
    "\n",
    "One artefact to be included in your submission is a python library structured according to the \"Access, Assess, Address\" standard for data science solutions. You will submit this library alongside your code. Use the cell below to perform the necessary installation instructions for your library.\n",
    "\n",
    "You should base your module on the template repository given by the Fynesse template repository. That should make it `pip` installable as below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f71cb7",
   "metadata": {
    "id": "13f71cb7"
   },
   "outputs": [],
   "source": [
    "# Install your library here, for example the fynesse template \n",
    "# is set up to be pip installable\n",
    "%pip install git+https://github.com/lawrennd/fynesse_template.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fafca0",
   "metadata": {
    "id": "c2fafca0"
   },
   "source": [
    "Your own library should be installed in the line above, then you can import it as usual (where you can either replace `fynesse` with the name you've given your analysis module or you can leave the name as `fynesse` as you prefer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db162b53",
   "metadata": {
    "id": "db162b53"
   },
   "outputs": [],
   "source": [
    "import fynesse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26533cf6",
   "metadata": {
    "id": "26533cf6"
   },
   "source": [
    "## Question 1. Accessing a Database of House Prices, Latitudes and Longitudes\n",
    "\n",
    "The UK price paid data for housing in dates back to 1995 and contains millions of transactions. The size of the data makes it unwieldy to manipulate directly in python frameworks such as `pandas`. As a result we will host the data in a *relational database*. \n",
    "\n",
    "Using the following ideas.\n",
    "\n",
    "1. A cloud hosted database (such as MariaDB hosted on the AWS RDS service).\n",
    "2. The SQL language wrapped in appropriately structured python code.\n",
    "3. Joining of two databases.\n",
    "\n",
    "You will construct a database containing tables that contain all house prices, latitudes and longitudes from the UK house price data base since 1995.\n",
    "\n",
    "You will likely find the following resources helpful.\n",
    "\n",
    "1. Lecture 1, 2 and 3.\n",
    "2. Lab class 1 and 2.\n",
    "3. The UK Price Paid data for houses: <https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads>\n",
    "4. The UK ONS Data base of postcode latitude and longitudes:  <https://www.getthedata.com/open-postcode-geo>\n",
    "\n",
    "Below we provide codeboxes and hints to help you develop your answer.\n",
    "\n",
    "*The main knowledge you need to do a first pass through this question will have been taught by the end of Lab Session 2 (11th November 2021). You will likely want to review your answer as part of **refactoring** your code  and analysis pipeline shortly before hand in.*\n",
    "\n",
    "*5 Marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd689312",
   "metadata": {
    "id": "fd689312"
   },
   "source": [
    "### Task A\n",
    "\n",
    "Set up the database. You'll need to set up a database on AWS. You were guided in how to do this in the lab sessions. You should be able to use the same database instance you created in the lab, or you can delete that and start with a fresh instance. You'll remember from the lab that the database requires credentials (username, password) to access. It's good practice to store those credentials *outside* the notebook so you don't accidentally share them by e.g. checking code into a repository. \n",
    "  \n",
    "Call the database you use for this assessment `property_prices`.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feaf09b9",
   "metadata": {
    "id": "feaf09b9"
   },
   "outputs": [],
   "source": [
    "# Write code for requesting and storing credentials (username, password) here.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_env(key: str) -> str:\n",
    "  val = os.getenv(key)\n",
    "  assert val is not None, f\"Please make sure to set the {key} environment variable, or put it in .env\"\n",
    "  return val\n",
    "\n",
    "# Helper class to make the type annotations clearer\n",
    "class Credentials(TypedDict):\n",
    "  username: str\n",
    "  password: str\n",
    "  host: str\n",
    "  database_name: str\n",
    "\n",
    "creds: Credentials = {\n",
    "  \"username\": get_env(\"ADS_DB_USERNAME\"),\n",
    "  \"password\": get_env(\"ADS_DB_PASSWORD\"),\n",
    "  \"host\": get_env(\"ADS_DB_HOST\"),\n",
    "  \"database_name\": \"property_prices\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036c419d",
   "metadata": {
    "id": "036c419d"
   },
   "outputs": [],
   "source": [
    "# Write any other setup code you need for setting up database access here.\n",
    "import pymysql\n",
    "from typing import List, Any\n",
    "from pymysql.connections import Connection\n",
    "\n",
    "def make_connection(creds: Credentials) -> pymysql.connections.Connection:\n",
    "  return pymysql.connect(\n",
    "    user=creds[\"username\"], \n",
    "    password=creds[\"password\"], \n",
    "    host=creds[\"host\"],\n",
    "    database=creds[\"database_name\"],\n",
    "    local_infile=True,\n",
    "  )\n",
    "\n",
    "def execute(conn: Connection, sql: str, *args: List[Any]) -> None:\n",
    "  with conn.cursor() as cur:\n",
    "    cur.execute(sql, args=args)\n",
    "  conn.commit()\n",
    "\n",
    "with make_connection({**creds, \"database_name\": \"mysql\"}) as conn:\n",
    "    execute(conn, \"CREATE DATABASE IF NOT EXISTS `property_prices` DEFAULT CHARACTER SET utf8 COLLATE utf8_bin;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7237f",
   "metadata": {
    "id": "33c7237f"
   },
   "source": [
    "### Task B\n",
    "\n",
    "Create a database table called `pp_data` containing all the UK Price Paid data from the [gov.uk site](https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads). You'll need to prepare a new table to receive the data and upload the UK Price Paid data to your database instance. The total data is over 3 gigabytes in size. We suggest that rather than downloading the full data in CSV format, you use the fact that they have split the data into years and into different parts per year. For example, the first part of the data for 2018 is stored at <http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2018-part1.csv>. Each of these files is less than 100MB and can be downloaded very quickly to local disk, then uploaded using \n",
    "\n",
    "\n",
    "```\n",
    "LOCAL DATA LOAD INFILE 'filename' INTO TABLE `table_name`\n",
    "FIELDS TERMINATED BY ',' \n",
    "LINES STARTING BY '' TERMINATED BY '\\n';\n",
    "```\n",
    "*Note* this command should be wrapped and placed in an appropriately structured python module. \n",
    "\n",
    "Each 'data part' should be downloadable from the `gov.uk` site and uploadable to your database instance in a couple of seconds. By looping across the years and different parts, you should be able to robustly upload this large data set to your database instance in a matter of minutes. \n",
    "\n",
    "You may find the following schema useful in creation of your database:\n",
    "\n",
    "```\n",
    "--\n",
    "-- Table structure for table `pp_data`\n",
    "--\n",
    "DROP TABLE IF EXISTS `pp_data`;\n",
    "CREATE TABLE IF NOT EXISTS `pp_data` (\n",
    "  `transaction_unique_identifier` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `price` int(10) unsigned NOT NULL,\n",
    "  `date_of_transfer` date NOT NULL,\n",
    "  `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "  `property_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `new_build_flag` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `tenure_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `primary_addressable_object_name` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `secondary_addressable_object_name` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `street` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `locality` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `town_city` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `district` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `county` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `ppd_category_type` varchar(2) COLLATE utf8_bin NOT NULL,\n",
    "  `record_status` varchar(2) COLLATE utf8_bin NOT NULL,\n",
    "  `db_id` bigint(20) unsigned NOT NULL\n",
    ") DEFAULT CHARSET=utf8 COLLATE=utf8_bin AUTO_INCREMENT=1 ;\n",
    "```\n",
    "This schema is written by Dale Potter and can be found on Github here: <https://github.com/dalepotter/uk_property_price_data/blob/master/create_db.sql>\n",
    "\n",
    "You may also find it helpful to set up the following primary key in the database\n",
    "\n",
    "```\n",
    "--\n",
    "-- Primary key for table `pp_data`\n",
    "--\n",
    "ALTER TABLE `pp_data`\n",
    "ADD PRIMARY KEY (`db_id`);\n",
    "MODIFY `db_id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=1;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e789b174",
   "metadata": {
    "id": "e789b174"
   },
   "source": [
    "In the box below, briefly describe what the schema is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9d674",
   "metadata": {
    "id": "d7d9d674"
   },
   "source": [
    "This schema contains data about property sales. It relates the price at which a property was sold to the location and date of sale. It does not contain geographical data such as GPS coordinates, but instead deals with administrative locations (address, post code, etc.). It also assigns a unique ID named `db_id` which increments from 1 with each row. This is used as the primary key for the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad93537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need for creating the database table and uploading the data here.\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def upload_file(conn: Connection, table: str, path: str) -> None:\n",
    "  # This is a potential SQL injection surface\n",
    "  # but there doesn't seem to be an easy way\n",
    "  # around it, as PyMySQL always escapes parameters\n",
    "  # with single quotes (') rather than backticks (`)\n",
    "  # which causes a syntax error when applied to\n",
    "  # table names\n",
    "  execute(\n",
    "    conn,\n",
    "    f\"\"\"LOAD DATA LOCAL INFILE %s\n",
    "INTO TABLE `{table}`\n",
    "FIELDS TERMINATED BY ','\n",
    "OPTIONALLY ENCLOSED BY '\"'\n",
    "LINES STARTING BY ''\n",
    "TERMINATED BY '\\n';\"\"\",\n",
    "    path\n",
    "  )\n",
    "\n",
    "def query(conn: Connection, sql: str, *args: List[Any]) -> Tuple[Tuple[Any]]:\n",
    "  with conn.cursor() as cur:\n",
    "    cur.execute(sql, *args)\n",
    "    return cur.fetchall()\n",
    "\n",
    "def download_file(url: str, output_path: str) -> None:\n",
    "  if os.path.exists(output_path):\n",
    "    print(f\"Skipping download of {output_path} because it already exists\")\n",
    "    return\n",
    "  response = requests.get(url)\n",
    "  with open(output_path, \"wb\") as fp:\n",
    "    fp.write(response.content.strip(b\"\\n\"))\n",
    "  print(f\"Downloaded {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92da8c96",
   "metadata": {
    "id": "92da8c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download of ./pp_data/y1995-part1.csv because it already exists\n",
      "Skipping download of ./pp_data/y1995-part2.csv because it already exists\n",
      "Skipping download of ./pp_data/y1996-part1.csv because it already exists\n",
      "Skipping download of ./pp_data/y1996-part2.csv because it already exists\n",
      "Skipping download of ./pp_data/y1997-part1.csv because it already exists\n",
      "Skipping download of ./pp_data/y1997-part2.csv because it already exists\n",
      "Skipping download of ./pp_data/y1998-part1.csv because it already exists\n",
      "Skipping download of ./pp_data/y1998-part2.csv because it already exists\n",
      "Skipping download of ./pp_data/y1999-part1.csv because it already exists\n",
      "Skipping download of ./pp_data/y1999-part2.csv because it already exists\n",
      "Skipping download of ./pp_data/y2000-part1.csv because it already exists\n",
      "Skipping download of ./pp_data/y2000-part2.csv because it already exists\n",
      "Skipping download of ./pp_data/y2001-part1.csv because it already exists\n",
      "Downloaded ./pp_data/y2001-part2.csv\n",
      "Downloaded ./pp_data/y2002-part1.csv\n",
      "Downloaded ./pp_data/y2002-part2.csv\n",
      "Downloaded ./pp_data/y2003-part1.csv\n",
      "Downloaded ./pp_data/y2003-part2.csv\n",
      "Downloaded ./pp_data/y2004-part1.csv\n",
      "Downloaded ./pp_data/y2004-part2.csv\n",
      "Downloaded ./pp_data/y2005-part1.csv\n",
      "Downloaded ./pp_data/y2005-part2.csv\n",
      "Downloaded ./pp_data/y2006-part1.csv\n",
      "Downloaded ./pp_data/y2006-part2.csv\n",
      "Downloaded ./pp_data/y2007-part1.csv\n",
      "Downloaded ./pp_data/y2007-part2.csv\n",
      "Downloaded ./pp_data/y2008-part1.csv\n",
      "Downloaded ./pp_data/y2008-part2.csv\n",
      "Downloaded ./pp_data/y2009-part1.csv\n",
      "Downloaded ./pp_data/y2009-part2.csv\n",
      "Downloaded ./pp_data/y2010-part1.csv\n",
      "Downloaded ./pp_data/y2010-part2.csv\n",
      "Downloaded ./pp_data/y2011-part1.csv\n",
      "Downloaded ./pp_data/y2011-part2.csv\n",
      "Downloaded ./pp_data/y2012-part1.csv\n",
      "Downloaded ./pp_data/y2012-part2.csv\n",
      "Downloaded ./pp_data/y2013-part1.csv\n",
      "Downloaded ./pp_data/y2013-part2.csv\n",
      "Downloaded ./pp_data/y2014-part1.csv\n",
      "Downloaded ./pp_data/y2014-part2.csv\n",
      "Downloaded ./pp_data/y2015-part1.csv\n",
      "Downloaded ./pp_data/y2015-part2.csv\n",
      "Downloaded ./pp_data/y2016-part1.csv\n",
      "Downloaded ./pp_data/y2016-part2.csv\n",
      "Downloaded ./pp_data/y2017-part1.csv\n",
      "Downloaded ./pp_data/y2017-part2.csv\n",
      "Downloaded ./pp_data/y2018-part1.csv\n",
      "Downloaded ./pp_data/y2018-part2.csv\n",
      "Downloaded ./pp_data/y2019-part1.csv\n",
      "Downloaded ./pp_data/y2019-part2.csv\n",
      "Downloaded ./pp_data/y2020-part1.csv\n",
      "Downloaded ./pp_data/y2020-part2.csv\n",
      "Downloaded ./pp_data/y2021-part1.csv\n",
      "Downloaded ./pp_data/y2021-part2.csv\n",
      "Downloaded ./pp_data/y2022.csv\n"
     ]
    }
   ],
   "source": [
    "def upload_pp_data_file(\n",
    "  conn: Connection,\n",
    "  year: int,\n",
    "  part: Optional[int] = None,\n",
    "  local_path_template: str = \"./pp_data/y%d%s.csv\",\n",
    "  url_template:str = \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-%d%s.csv\"\n",
    ") -> None:\n",
    "  part_str = \"\" if part is None else f\"-part{part}\"\n",
    "  url = url_template % (year, part_str)\n",
    "  local_path = local_path_template % (year, part_str)\n",
    "  download_file(url, local_path)\n",
    "  upload_file(conn, \"pp_data\", local_path)\n",
    "\n",
    "with make_connection(creds) as conn:\n",
    "  execute(conn, \"DROP TABLE IF EXISTS `pp_data`;\")\n",
    "  execute(conn, \"\"\"CREATE TABLE IF NOT EXISTS `pp_data` (\n",
    "    `transaction_unique_identifier` tinytext COLLATE utf8_bin NOT NULL,\n",
    "    `price` int(10) unsigned NOT NULL,\n",
    "    `date_of_transfer` date NOT NULL,\n",
    "    `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "    `property_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "    `new_build_flag` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "    `tenure_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "    `primary_addressable_object_name` tinytext COLLATE utf8_bin NOT NULL,\n",
    "    `secondary_addressable_object_name` tinytext COLLATE utf8_bin NOT NULL,\n",
    "    `street` tinytext COLLATE utf8_bin NOT NULL,\n",
    "    `locality` tinytext COLLATE utf8_bin NOT NULL,\n",
    "    `town_city` tinytext COLLATE utf8_bin NOT NULL,\n",
    "    `district` tinytext COLLATE utf8_bin NOT NULL,\n",
    "    `county` tinytext COLLATE utf8_bin NOT NULL,\n",
    "    `ppd_category_type` varchar(2) COLLATE utf8_bin NOT NULL,\n",
    "    `record_status` varchar(2) COLLATE utf8_bin NOT NULL,\n",
    "    `db_id` bigint(20) unsigned NOT NULL\n",
    "  ) DEFAULT CHARSET=utf8 COLLATE=utf8_bin AUTO_INCREMENT=1 ;\"\"\")\n",
    "  execute(conn, \"ALTER TABLE `pp_data` ADD PRIMARY KEY (`db_id`);\")\n",
    "  execute(conn, \"ALTER TABLE `pp_data` MODIFY `db_id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=1;\")\n",
    "\n",
    "  for year in range(1995, 2022):\n",
    "    for part in [1,2]:\n",
    "      upload_pp_data_file(conn, year, part)\n",
    "  upload_pp_data_file(conn, 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f5120a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting to find 27641648 records...\n",
      "Actually found 27641648 records!\n"
     ]
    }
   ],
   "source": [
    "print(\"Expecting to find \", end=\"\")\n",
    "!echo -n $(expr $(wc -l ./pp_data/*.csv | tail -n1 | sed 's/[^0-9]//g') + $(ls -1 ./pp_data | wc -l))\n",
    "print(\" records...\")\n",
    "\n",
    "with make_connection(creds) as conn:\n",
    "  num_records = query(conn, \"SELECT COUNT(*) FROM `pp_data`\")[0][0]\n",
    "  print(f\"Actually found {num_records} records!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ece66",
   "metadata": {
    "id": "bb9ece66"
   },
   "source": [
    "### Task C\n",
    "\n",
    "Create a database table called `postcode_data` containing the ONS Postcode information. <GetTheData.com> has organised data derived from the UK Office for National Statistics into a convenient CSV file. You can find details [here](https://www.getthedata.com/open-postcode-geo).\n",
    "\n",
    "\n",
    "The data you need can be found at this url: <https://www.getthedata.com/downloads/open_postcode_geo.csv.zip>. It will need to be unzipped before use.\n",
    "\n",
    "You may find the following schema useful for the postcode data (developed by Christian and Neil)\n",
    "\n",
    "```\n",
    "USE `property_prices`;\n",
    "--\n",
    "-- Table structure for table `postcode_data`\n",
    "--\n",
    "DROP TABLE IF EXISTS `postcode_data`;\n",
    "CREATE TABLE IF NOT EXISTS `postcode_data` (\n",
    "  `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "  `status` enum('live','terminated') NOT NULL,\n",
    "  `usertype` enum('small', 'large') NOT NULL,\n",
    "  `easting` int unsigned,\n",
    "  `northing` int unsigned,\n",
    "  `positional_quality_indicator` int NOT NULL,\n",
    "  `country` enum('England', 'Wales', 'Scotland', 'Northern Ireland', 'Channel Islands', 'Isle of Man') NOT NULL,\n",
    "  `lattitude` decimal(11,8) NOT NULL,\n",
    "  `longitude` decimal(10,8) NOT NULL,\n",
    "  `postcode_no_space` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `postcode_fixed_width_seven` varchar(7) COLLATE utf8_bin NOT NULL,\n",
    "  `postcode_fixed_width_eight` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "  `postcode_area` varchar(2) COLLATE utf8_bin NOT NULL,\n",
    "  `postcode_district` varchar(4) COLLATE utf8_bin NOT NULL,\n",
    "  `postcode_sector` varchar(6) COLLATE utf8_bin NOT NULL,\n",
    "  `outcode` varchar(4) COLLATE utf8_bin NOT NULL,\n",
    "  `incode` varchar(3)  COLLATE utf8_bin NOT NULL,\n",
    "  `db_id` bigint(20) unsigned NOT NULL\n",
    ") DEFAULT CHARSET=utf8 COLLATE=utf8_bin;\n",
    "```\n",
    "\n",
    "And you can load the CSV file into the table in one \"INFILE\".\n",
    "\n",
    "```\n",
    "LOAD DATA LOCAL INFILE 'open_postcode_geo.csv' INTO TABLE `postcode_data`\n",
    "FIELDS TERMINATED BY ',' \n",
    "LINES STARTING BY '' TERMINATED BY '\\n';\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5ecf5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded postcode_data/open_postcode_geo.csv.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "with make_connection(creds) as conn:\n",
    "  execute(conn, \"DROP TABLE IF EXISTS `postcode_data`;\")\n",
    "  execute(conn, \"\"\"CREATE TABLE IF NOT EXISTS `postcode_data` (\n",
    "  `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "  `status` enum('live','terminated') NOT NULL,\n",
    "  `usertype` enum('small', 'large') NOT NULL,\n",
    "  `easting` int unsigned,\n",
    "  `northing` int unsigned,\n",
    "  `positional_quality_indicator` int NOT NULL,\n",
    "  `country` enum('England', 'Wales', 'Scotland', 'Northern Ireland', 'Channel Islands', 'Isle of Man') NOT NULL,\n",
    "  `lattitude` decimal(11,8) NOT NULL,\n",
    "  `longitude` decimal(10,8) NOT NULL,\n",
    "  `postcode_no_space` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `postcode_fixed_width_seven` varchar(7) COLLATE utf8_bin NOT NULL,\n",
    "  `postcode_fixed_width_eight` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "  `postcode_area` varchar(2) COLLATE utf8_bin NOT NULL,\n",
    "  `postcode_district` varchar(4) COLLATE utf8_bin NOT NULL,\n",
    "  `postcode_sector` varchar(6) COLLATE utf8_bin NOT NULL,\n",
    "  `outcode` varchar(4) COLLATE utf8_bin NOT NULL,\n",
    "  `incode` varchar(3)  COLLATE utf8_bin NOT NULL,\n",
    "  `db_id` bigint(20) unsigned NOT NULL\n",
    ") DEFAULT CHARSET=utf8 COLLATE=utf8_bin;\"\"\")\n",
    "  execute(conn, \"ALTER TABLE `postcode_data` ADD PRIMARY KEY (`db_id`);\")\n",
    "  execute(conn, \"ALTER TABLE `postcode_data` MODIFY `db_id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=1;\")\n",
    "  execute(conn, \"CREATE INDEX idx_postcode_2 ON `postcode_data` (`postcode`);\")\n",
    "  execute(conn, \"CREATE INDEX idx_lattitude ON `postcode_data` (`lattitude`);\")\n",
    "  execute(conn, \"CREATE INDEX idx_longitude ON `postcode_data` (`longitude`);\")\n",
    "\n",
    "  postcode_data_dir = \"postcode_data\"\n",
    "  zip_path = os.path.join(postcode_data_dir, \"open_postcode_geo.csv.zip\")\n",
    "  csv_path = os.path.join(postcode_data_dir, \"open_postcode_geo.csv\")\n",
    "  \n",
    "  download_file(\"https://www.getthedata.com/downloads/open_postcode_geo.csv.zip\", zip_path)\n",
    "  with zipfile.ZipFile(zip_path, \"r\") as zfp:\n",
    "    zfp.extractall(postcode_data_dir)\n",
    "\n",
    "  upload_file(conn, \"postcode_data\", csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ca4ef",
   "metadata": {
    "id": "d67ca4ef"
   },
   "source": [
    "### Task D \n",
    "\n",
    "These data can now be joined to form a new table that contains house price paid and latitude longitude of the house. We could create a new table that contains all this information. However, the computation of that table will take some time because of the size of the two existing tables in the join. \n",
    "\n",
    "Instead, we're going to exploit the nature of the task. To build our prediction model, we're going to use the prices for a particular region in a given time period. This means we can select that region and time period and build the joined data only from the relevent rows from the two tables. This will save time on the join.\n",
    "\n",
    "Whether this is a good idea or not in a live system will depend on how often these predictions are required. If it's very often, it would likely be better to store the entired database joined, because the one-off cost for that join is amortised across all the future predictions. If only a few predictions are required (like in our lab class) then doing that join on the fly might be better. In that case you can make use of an  *inner join* for this data set creation.\n",
    "\n",
    "```\n",
    "USE `property_prices`;\n",
    "--\n",
    "-- Table structure for table `prices_coordinates_data`\n",
    "--\n",
    "DROP TABLE IF EXISTS `prices_coordinates_data`;\n",
    "CREATE TABLE IF NOT EXISTS `prices_coordinates_data` (\n",
    "  `price` int(10) unsigned NOT NULL,\n",
    "  `date_of_transfer` date NOT NULL,\n",
    "  `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "  `property_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `new_build_flag` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `tenure_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `locality` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `town_city` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `district` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `county` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `country` enum('England', 'Wales', 'Scotland', 'Northern Ireland', 'Channel Islands', 'Isle of Man') NOT NULL,\n",
    "  `lattitude` decimal(11,8) NOT NULL,\n",
    "  `longitude` decimal(10,8) NOT NULL,\n",
    "  `db_id` bigint(20) unsigned NOT NULL\n",
    ") DEFAULT CHARSET=utf8 COLLATE=utf8_bin AUTO_INCREMENT=1 ;\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7797f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dateutil\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class Box:\n",
    "  lat: float\n",
    "  lng: float\n",
    "  min_lat: float\n",
    "  max_lat: float\n",
    "  min_lng: float\n",
    "  max_lng: float\n",
    "  size_km: float\n",
    "  date: datetime.datetime\n",
    "  start_date: datetime.datetime\n",
    "  end_date: datetime.datetime\n",
    "  size_days: int\n",
    "\n",
    "def calculate_box(lat: float, lng: float, size_km: float, date: datetime.datetime, size_days: int) -> Box:\n",
    "  # Approximation based on https://en.wikipedia.org/wiki/Latitude#Meridian_distance_on_the_ellipsoid\n",
    "  size_lat_deg = size_km / 110.574\n",
    "  size_lng_deg = size_km / (111.320 * np.cos(np.deg2rad(lat)))\n",
    "\n",
    "  date = date.astimezone(datetime.timezone.utc)\n",
    "\n",
    "  return Box(\n",
    "    lat,\n",
    "    lng,\n",
    "    lat - size_lat_deg/2,\n",
    "    lat + size_lat_deg/2,\n",
    "    lng - size_lng_deg/2,\n",
    "    lng + size_lng_deg/2,\n",
    "    size_km,\n",
    "    date,\n",
    "    date + dateutil.relativedelta.relativedelta(days=-size_days),\n",
    "    date + dateutil.relativedelta.relativedelta(days=size_days),\n",
    "    size_days\n",
    "  )\n",
    "\n",
    "def join_pp_and_postcode_data(\n",
    "  conn: Connection,\n",
    "  box: Box\n",
    ") -> None:\n",
    "  execute(conn, \"DROP TABLE IF EXISTS `prices_coordinates_data`;\")\n",
    "  execute(conn, \"\"\"CREATE TABLE IF NOT EXISTS `prices_coordinates_data` (\n",
    "  `price` int(10) unsigned NOT NULL,\n",
    "  `date_of_transfer` date NOT NULL,\n",
    "  `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "  `property_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `new_build_flag` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `tenure_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `locality` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `town_city` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `district` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `county` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `country` enum('England', 'Wales', 'Scotland', 'Northern Ireland', 'Channel Islands', 'Isle of Man') NOT NULL,\n",
    "  `lattitude` decimal(11,8) NOT NULL,\n",
    "  `longitude` decimal(10,8) NOT NULL,\n",
    "  `db_id` bigint(20) unsigned NOT NULL\n",
    ") DEFAULT CHARSET=utf8 COLLATE=utf8_bin AUTO_INCREMENT=1 ;\"\"\")\n",
    "  execute(conn, \"ALTER TABLE `prices_coordinates_data` ADD PRIMARY KEY (`db_id`);\")\n",
    "  execute(conn, \"ALTER TABLE `prices_coordinates_data` MODIFY `db_id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=1;\")\n",
    "\n",
    "  execute(\n",
    "    conn,\n",
    "    \"\"\"INSERT INTO `prices_coordinates_data`\n",
    "SELECT `price`, `date_of_transfer`, `pp_data`.`postcode`, `property_type`, `new_build_flag`, `tenure_type`, `locality`, `town_city`, `district`, `county`, `country`, `lattitude`, `longitude`, 0\n",
    "FROM `pp_data`\n",
    "INNER JOIN `postcode_data`\n",
    "ON `pp_data`.`postcode`=`postcode_data`.`postcode`\n",
    "WHERE `lattitude` > %s\n",
    "AND `lattitude` < %s\n",
    "AND `longitude` > %s\n",
    "AND `longitude` < %s\n",
    "AND `date_of_transfer` > %s\n",
    "AND `date_of_transfer` < %s\"\"\",\n",
    "    box.min_lat,\n",
    "    box.max_lat,\n",
    "    box.min_lng,\n",
    "    box.max_lng,\n",
    "    box.start_date,\n",
    "    box.end_date\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4fecbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = calculate_box(\n",
    "  51.509865,\n",
    "  -0.118092,\n",
    "  10,\n",
    "  datetime.datetime(2020, 1, 1, tzinfo=datetime.timezone.utc),\n",
    "  5\n",
    ") # London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68d031b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with make_connection(creds) as conn:\n",
    "  join_pp_and_postcode_data(\n",
    "    conn,\n",
    "    box\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df36e5d1",
   "metadata": {
    "id": "df36e5d1"
   },
   "source": [
    "## Question 2. Accessing OpenStreetMap and Assessing the Available Features\n",
    "\n",
    "In question 3 you will be given the task of constructing a prediction system for property price levels at a given location. We expect that knowledge of the local region around the property should be helpful in making those price predictions. To evaluate this we will now look at [OpenStreetMap](https://www.openstreetmap.org) as a data source.\n",
    "\n",
    "The tasks below will guide you in accessing and assessing the OpenStreetMap data. The code you write will eventually be assimilated in your python module, but documentation of what you've included and why should remain in the notebook below. \n",
    "\n",
    "Accessing OpenStreetMap through its API can be done using the python library `osmx`. Using what you have learned about the `osmx` interface in the lectures, write general code for downloading points of interest and other relevant information that you believe may be useful for predicting house prices. Remembering the perspectives we've taken on *data science as debugging*, the remarks we've made when discussing *the data crisis* of the importance of reusability in data analysis, and the techniques we've explored in the labsessions for visualising features and exploring their correlation use the notebook to document your assessment of the OpenStreetMap data as a potential source of data.\n",
    "\n",
    "The knowledge you need to do a first pass through this question will have been taught by end of lab session three (16th November 2021). You will likely want to review your answer as part of *refactoring* your code and analysis pipeline shortly before hand in.\n",
    "\n",
    "You should write reusable code that allows you to explore the characteristics of different points of interest. Looking ahead to question 3 you'll want to incorporate these points of interest in your prediction code.\n",
    "\n",
    "*5 marks*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "042a2863",
   "metadata": {
    "id": "042a2863"
   },
   "outputs": [],
   "source": [
    "# Use this cell and cells below for summarising your analysis and documenting your decision making.\n",
    "\n",
    "import osmnx as ox\n",
    "from geopandas import GeoDataFrame\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "ox.settings.use_cache = True\n",
    "\n",
    "default_tags = {\n",
    "  \"amenity\": True, \n",
    "  \"building\": True, \n",
    "  \"historic\": True, \n",
    "  \"leisure\": True, \n",
    "  \"shop\": True, \n",
    "  \"tourism\": True,\n",
    "  \"cuisine\": True\n",
    "}\n",
    "def fetch_pois(box: Box, tags: Dict[str, bool]=default_tags, pbar: Optional[tqdm] = None) -> Dict[datetime.datetime, GeoDataFrame]:\n",
    "  result: Dict[datetime.datetime, GeoDataFrame] = {}\n",
    "  date = box.start_date\n",
    "  while date <= box.end_date:\n",
    "    overpass_settings = ox.settings.overpass_settings\n",
    "    if date is not None:\n",
    "      ox.settings.overpass_settings = overpass_settings + f'[date:\"{date.astimezone(datetime.timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")}\"]'\n",
    "    result_for_date = ox.geometries_from_bbox(box.max_lat, box.min_lat, box.min_lng, box.max_lng, tags)\n",
    "    ox.settings.overpass_settings = overpass_settings\n",
    "    result[date] = result_for_date\n",
    "    date += dateutil.relativedelta.relativedelta(days=1)\n",
    "    if pbar is not None:\n",
    "      pbar.update()\n",
    "  return result\n",
    "\n",
    "def count_pois(pois: Dict[datetime.datetime, GeoDataFrame], box: Box) -> Dict[datetime.datetime, Dict[str, int]]:\n",
    "  result: Dict[datetime.datetime, Dict[str, int]] = {}\n",
    "  for date in pois:\n",
    "    if date < box.start_date or date > box.end_date:\n",
    "      continue\n",
    "    pois_date = pois[date]\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "      warnings.simplefilter(\"ignore\")\n",
    "      lat_series = pois_date[\"geometry\"].centroid.y\n",
    "      lng_series = pois_date[\"geometry\"].centroid.x\n",
    "      \n",
    "    pois_date_area = pois_date[(box.min_lat <= lat_series) & (lat_series <= box.max_lat) & (box.min_lng <= lng_series) & (lng_series <= box.max_lng)]\n",
    "    result_for_date: Dict[str, int] = {}\n",
    "    for tag in tags:\n",
    "      result_for_date[tag] = pois_date_area[tag].notna().sum()\n",
    "    result[date] = result_for_date\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e85de9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>City</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.51498200</td>\n",
       "      <td>-0.11486300</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>F</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>2310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.51797200</td>\n",
       "      <td>-0.13047600</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>F</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.52582200</td>\n",
       "      <td>-0.12577500</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>F</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.48926500</td>\n",
       "      <td>-0.15460100</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>F</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>8540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.52203100</td>\n",
       "      <td>-0.07879400</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>F</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>1254400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Lattitude    Longitude        Date Type    City    Price\n",
       "0  51.51498200  -0.11486300  2019-12-30    F  LONDON  2310000\n",
       "1  51.51797200  -0.13047600  2019-12-30    F  LONDON  1600000\n",
       "2  51.52582200  -0.12577500  2019-12-30    F  LONDON   460000\n",
       "3  51.48926500  -0.15460100  2019-12-30    F  LONDON  8540000\n",
       "4  51.52203100  -0.07879400  2019-12-30    F  LONDON  1254400"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with make_connection(creds) as conn:\n",
    "  records = query(conn, \"SELECT lattitude, longitude, date_of_transfer, property_type, town_city, price FROM prices_coordinates_data\")\n",
    "  df = pd.DataFrame(records, columns=[\"Lattitude\", \"Longitude\", \"Date\", \"Type\", \"City\", \"Price\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "660a24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    }
   ],
   "source": [
    "dilation_km = 0.5\n",
    "\n",
    "dilated_box = calculate_box(\n",
    "  box.lat,\n",
    "  box.lng,\n",
    "  box.size_km+dilation_km,\n",
    "  box.date,\n",
    "  box.size_days\n",
    ")\n",
    "\n",
    "tags = {\"cuisine\": True, \"shop\": True, \"public_transport\": True}\n",
    "\n",
    "with tqdm(total=dilated_box.size_days*2+1, leave=False) as pbar:\n",
    "  pois = fetch_pois(dilated_box, tags, pbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1c1ba366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_km(lat1: float, lng1: float, lat2: float, lng2: float) -> float:\n",
    "  # Approximation from https://www.omnicalculator.com/other/latitude-longitude-distance#obtaining-the-distance-between-two-points-on-earth-distance-between-coordinates\n",
    "\n",
    "  R = 6371\n",
    "\n",
    "  lat1 = np.deg2rad(lat1)\n",
    "  lat2 = np.deg2rad(lat2)\n",
    "  lng1 = np.deg2rad(lng1)\n",
    "  lng2 = np.deg2rad(lng2)\n",
    "\n",
    "  return 2 * R * np.arcsin(\n",
    "    np.sqrt(\n",
    "      np.sin((lat2-lat1)/2) ** 2 +\n",
    "      (\n",
    "        np.cos(lat1) *\n",
    "        np.cos(lat2) *\n",
    "        np.sin((lng2 - lng1)/2)**2\n",
    "      )\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f3fceefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>City</th>\n",
       "      <th>Price</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>shop</th>\n",
       "      <th>public_transport</th>\n",
       "      <th>Distance to City Center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.51498200</td>\n",
       "      <td>-0.11486300</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>F</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>2310000</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>1.654265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.51797200</td>\n",
       "      <td>-0.13047600</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>F</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>1600000</td>\n",
       "      <td>49</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>2.429078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.52582200</td>\n",
       "      <td>-0.12577500</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>F</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>460000</td>\n",
       "      <td>23</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>3.035119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.48926500</td>\n",
       "      <td>-0.15460100</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>F</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>8540000</td>\n",
       "      <td>6</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>3.368204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.52203100</td>\n",
       "      <td>-0.07879400</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>F</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>1254400</td>\n",
       "      <td>28</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>3.197568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Lattitude    Longitude        Date Type    City    Price  cuisine  shop  \\\n",
       "0  51.51498200  -0.11486300  2019-12-30    F  LONDON  2310000       20    18   \n",
       "1  51.51797200  -0.13047600  2019-12-30    F  LONDON  1600000       49   128   \n",
       "2  51.52582200  -0.12577500  2019-12-30    F  LONDON   460000       23    68   \n",
       "3  51.48926500  -0.15460100  2019-12-30    F  LONDON  8540000        6    69   \n",
       "4  51.52203100  -0.07879400  2019-12-30    F  LONDON  1254400       28    62   \n",
       "\n",
       "   public_transport  Distance to City Center  \n",
       "0                11                 1.654265  \n",
       "1                22                 2.429078  \n",
       "2                 6                 3.035119  \n",
       "3                10                 3.368204  \n",
       "4                27                 3.197568  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_predictors = df.copy()\n",
    "\n",
    "counts = {}\n",
    "for tag in tags:\n",
    "  counts[tag] = []\n",
    "\n",
    "distance_to_city_center = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0], leave=False):\n",
    "  lat = float(row[\"Lattitude\"])\n",
    "  lng = float(row[\"Longitude\"])\n",
    "  date = datetime.datetime.combine(row[\"Date\"], datetime.datetime.min.time(), tzinfo=datetime.timezone.utc)\n",
    "  city = row[\"City\"]\n",
    "\n",
    "  poi_counts = count_pois(pois, calculate_box(lat, lng, dilation_km, date, 0))\n",
    "\n",
    "  for tag in tags:\n",
    "    counts[tag].append(poi_counts[date][tag])\n",
    "  \n",
    "  with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    city_center = ox.geocode_to_gdf(city).centroid\n",
    "  distance_to_city_center.append(dist_km(lat, lng, city_center.y[0], city_center.x[0]))\n",
    "\n",
    "\n",
    "for tag in tags:\n",
    "  df_with_predictors[tag] = counts[tag]\n",
    "df_with_predictors[\"Distance to City Center\"] = distance_to_city_center\n",
    "\n",
    "df_with_predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2c8d55e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f19626098e0>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(df_with_predictors[\"shop\"], np.log(df_with_predictors[\"Price\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0e365",
   "metadata": {
    "id": "09a0e365"
   },
   "source": [
    "## Question 3. Addressing a Property Price Prediction Question\n",
    "\n",
    "For your final tick, we will be asking you to make house price predictions for a given location, date and property type in the UK. You will provide a function that takes input a latitude and longitude as well as the `property_type` (either type\" of property (either `F` - flat, `S` - semidetached, `D` - detached, `T` - terraced or `O` other). Create this function in the `address.py` file, for example in the form,\n",
    "\n",
    "```\n",
    "def predict_price(latitude, longitude, date, property_type):\n",
    "    \"\"\"Price prediction for UK housing.\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "We suggest that you use the following approach when building your prediction. \n",
    "\n",
    "1. Select a bounding box around the housing location in latitude and longitude.\n",
    "2. Select a data range around the prediction date. \n",
    "3. Use the data ecosystem you have build above to build a training set from the relevant time period and location in the UK. Include appropriate features from OSM to improve the prediction.\n",
    "4. Train a linear model on the data set you have created.\n",
    "5. Validate the quality of the model.\n",
    "6. Provide a prediction of the price from the model, warning appropriately if your validation indicates the quality of the model is poor.\n",
    "\n",
    "The knowledge you need to do a first pass through this question will have been taught by end of lab session four (25th November 2021). You will likely want to review your answer as part of *refactoring* your code shortly before hand in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "Si3K0A2zM7pa",
   "metadata": {
    "id": "Si3K0A2zM7pa"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from typing import Iterable\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def calculate_days_since_1995(dates: Iterable[datetime.datetime]) -> List[int]:\n",
    "  result: List[int] = []\n",
    "  for date in dates:\n",
    "    result.append((date - datetime.date(1995, 1, 1)).days)\n",
    "  return result\n",
    "\n",
    "def calculate_one_hot_from_property_type(property_types: Iterable[str]) -> Tuple[List[int]]:\n",
    "  flats: List[int] = []\n",
    "  semi_detached: List[int] = []\n",
    "  detached: List[int] = []\n",
    "  terraced: List[int] = []\n",
    "  other: List[int] = []\n",
    "\n",
    "  for t in property_types:\n",
    "    flats.append(1 if t == \"F\" else 0)\n",
    "    semi_detached.append(1 if t == \"S\" else 0)\n",
    "    detached.append(1 if t == \"D\" else 0)\n",
    "    terraced.append(1 if t == \"T\" else 0)\n",
    "    other.append(1 if t == \"O\" else 0)\n",
    "\n",
    "  return (flats, semi_detached, detached, terraced, other)\n",
    "\n",
    "def get_pca_transform(design: np.ndarray) -> PCA:\n",
    "  return PCA(n_components=design.shape[1]).fit(design)\n",
    "\n",
    "def make_design(\n",
    "  lat: Iterable[float],\n",
    "  lng: Iterable[float],\n",
    "  date: Iterable[datetime.date],\n",
    "  property_type: Iterable[str],\n",
    "  cuisine: Iterable[int],\n",
    "  shop: Iterable[int],\n",
    "  public_transport: Iterable[int],\n",
    "  distance_to_city_center: Iterable[float],\n",
    "  pca: Optional[PCA] = None\n",
    ") -> np.ndarray:\n",
    "  x = np.stack((\n",
    "    lat,\n",
    "    lng,\n",
    "    calculate_days_since_1995(date),\n",
    "    *calculate_one_hot_from_property_type(property_type),\n",
    "    cuisine,\n",
    "    shop,\n",
    "    public_transport,\n",
    "    distance_to_city_center,\n",
    "    np.ones_like(lat)\n",
    "  )).astype(float).T\n",
    "  if pca is not None:\n",
    "    return pca.transform(x)\n",
    "  return x\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "632e9974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218924000.53092176 0.313953488372093\n",
      "22039924.684261117 0.4090909090909091\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   86\n",
      "Model:                            GLM   Df Residuals:                       74\n",
      "Model Family:                   Gamma   Df Model:                           11\n",
      "Link Function:                    Log   Scale:                          1.0387\n",
      "Method:                          IRLS   Log-Likelihood:                -1322.1\n",
      "Date:                Wed, 23 Nov 2022   Deviance:                       76.560\n",
      "Time:                        17:51:05   Pearson chi2:                     76.9\n",
      "No. Iterations:                   100   Pseudo R-squ. (CS):             0.8793\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             1.8629      5.805      0.321      0.748      -9.514      13.240\n",
      "x2            -8.1784      3.330     -2.456      0.014     -14.704      -1.652\n",
      "x3            -0.1877      0.101     -1.864      0.062      -0.385       0.010\n",
      "x4           270.9079    160.651      1.686      0.092     -43.963     585.779\n",
      "x5           272.4436    160.666      1.696      0.090     -42.456     587.343\n",
      "x6           271.8883    160.742      1.691      0.091     -43.159     586.936\n",
      "x7           271.8032    160.673      1.692      0.091     -43.109     586.716\n",
      "x8           273.9346    160.569      1.706      0.088     -40.774     588.643\n",
      "x9            -0.0269      0.015     -1.743      0.081      -0.057       0.003\n",
      "x10            0.0186      0.006      3.324      0.001       0.008       0.030\n",
      "x11            0.0306      0.015      1.985      0.047       0.000       0.061\n",
      "x12           -0.3546      0.123     -2.872      0.004      -0.597      -0.113\n",
      "const       1360.9776    803.295      1.694      0.090    -213.452    2935.408\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1729)\n",
    "\n",
    "x_train_val = (\n",
    "  np.array(df_with_predictors[\"Lattitude\"]),\n",
    "  np.array(df_with_predictors[\"Longitude\"]),\n",
    "  np.array(df_with_predictors[\"Date\"]),\n",
    "  np.array(df_with_predictors[\"Type\"]),\n",
    "  np.array(df_with_predictors[\"cuisine\"]),\n",
    "  np.array(df_with_predictors[\"shop\"]),\n",
    "  np.array(df_with_predictors[\"public_transport\"]),\n",
    "  np.array(df_with_predictors[\"Distance to City Center\"])\n",
    ")\n",
    "y_train_val = np.array(df_with_predictors[\"Price\"], dtype=float)\n",
    "\n",
    "train_prop = 0.8\n",
    "train_val_num = len(x_train_val[0])\n",
    "train_num = int(train_val_num * train_prop)\n",
    "train_val_idxs = np.arange(0, train_val_num)\n",
    "np.random.shuffle(train_val_idxs)\n",
    "train_idxs = train_val_idxs[:train_num]\n",
    "val_idxs = train_val_idxs[train_num:]\n",
    "\n",
    "x_train = tuple(x_[train_idxs] for x_ in x_train_val)\n",
    "x_val = tuple(x_[val_idxs] for x_ in x_train_val)\n",
    "\n",
    "y_train = y_train_val[train_idxs]\n",
    "y_val = y_train_val[val_idxs]\n",
    "\n",
    "design_train = make_design(*x_train)\n",
    "pca = None # get_pca_transform(design_train)\n",
    "# design_train = pca.transform(design_train)\n",
    "\n",
    "m_linear_basis = sm.GLM(y_train, design_train, family=sm.families.Gamma(link=sm.families.links.Log()))\n",
    "results_basis = m_linear_basis.fit()\n",
    "\n",
    "design_val = make_design(*x_val, pca)\n",
    "\n",
    "ci = 0.95\n",
    "\n",
    "y_train_pred = results_basis.get_prediction(design_train).summary_frame(alpha=1-ci)\n",
    "y_val_pred = results_basis.get_prediction(design_val).summary_frame(alpha=1-ci)\n",
    "\n",
    "train_rmse = np.sqrt(np.sum((y_train_pred[\"mean\"] - y_train) ** 2))\n",
    "train_proportion_in_ci = np.mean((y_train_pred[\"mean_ci_lower\"] >= y_train) & (y_train <= y_train_pred[\"mean_ci_upper\"]))\n",
    "\n",
    "val_rmse = np.sqrt(np.sum((y_val_pred[\"mean\"] - y_val) ** 2))\n",
    "val_proportion_in_ci = np.mean((y_val_pred[\"mean_ci_lower\"] >= y_val) & (y_val <= y_val_pred[\"mean_ci_upper\"]))\n",
    "\n",
    "print(train_rmse, train_proportion_in_ci)\n",
    "print(val_rmse, val_proportion_in_ci)\n",
    "\n",
    "print(results_basis.summary())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
